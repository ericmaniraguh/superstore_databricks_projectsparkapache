{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c233e7-5bd4-4b1e-84a1-a4041de210e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Retail Sales Project ‚Äî Databricks PySpark Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5230c5dd-ac4f-4930-a114-2ff9c5118c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##  Retail Sales Project ‚Äî Databricks PySpark Implementation\n",
    "\n",
    "###  Objective\n",
    "\n",
    "The goal of this project is to analyze **Superstore retail sales data** using **PySpark in Databricks**,  \n",
    "and build an interactive analytical dashboard that provides **monthly and yearly business insights** to stakeholders.\n",
    "\n",
    "> Since the dataset represents historical data (e.g., past years), the notebook focuses on **trend analysis** rather than live weekly refresh.\n",
    "\n",
    "---\n",
    "\n",
    "###  Business Requirements\n",
    "\n",
    "Stakeholders want to monitor and analyze the following **key business metrics**:\n",
    "\n",
    "| Metric ID | Description |\n",
    "|------------|--------------|\n",
    "| 1 | Total number of unique customers |\n",
    "| 2 | Total number of orders |\n",
    "| 3 | Total sales amount (for the selected period) |\n",
    "| 4 | Total profit |\n",
    "| 5 | Top sales by country |\n",
    "| 6 | Most profitable region and country |\n",
    "| 7 | Top sales by product category |\n",
    "| 8 | Top 10 products by sub-category |\n",
    "| 9 | Most frequently ordered product (by quantity) |\n",
    "| 10 | Top customer based on total sales per city |\n",
    "\n",
    "**Note:**  \n",
    "The Databricks notebook supports **manual refresh and dynamic filtering** by **date range**, **month**, and **year** using widgets.  \n",
    "Future automation (e.g., weekly/monthly updates) can be enabled if live data is introduced.\n",
    "\n",
    "---\n",
    "\n",
    "###  Project Workflow\n",
    "\n",
    "```plaintext\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ   One-Pager Req.     ‚îÇ  ‚Üê (Business / PM / Stakeholders)\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ      Data Team       ‚îÇ  ‚Üê (Data Engineers receive requirement)\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ  Data Understanding  ‚îÇ  ‚Üê (Schema review, profiling, cleaning)\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                  ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ   Dev / QA / Prod    ‚îÇ  ‚Üê (Pipeline deployment in Databricks)\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "###  Technical Stack\n",
    "\n",
    "| Layer           | Tool / Technology                        |\n",
    "| --------------- | ---------------------------------------- |\n",
    "| Data Processing | **Apache Spark (PySpark)**               |\n",
    "| Platform        | **Databricks**                           |\n",
    "| Data Storage    | **Delta Lake / Parquet**                 |\n",
    "| Visualization   | **Databricks SQL / Power BI / Tableau**  |\n",
    "| Scheduling      | **Databricks Jobs / Airflow (optional)** |\n",
    "| Version Control | **GitHub**                               |\n",
    "| Environment     | **Dev ‚Üí QA ‚Üí Prod**                      |\n",
    "\n",
    "---\n",
    "\n",
    "###  Data Understanding\n",
    "\n",
    "**Dataset Schema:**\n",
    "\n",
    "| Column Name   | Data Type | Description                                |\n",
    "| ------------- | --------- | ------------------------------------------ |\n",
    "| ID            | Integer   | Record identifier                          |\n",
    "| Order_ID      | String    | Unique order identifier                    |\n",
    "| Order_Date    | String    | Order date (string, to be cast as `date`)  |\n",
    "| Ship_Date     | Date      | Date the order was shipped                 |\n",
    "| Ship_Mode     | String    | Shipping method                            |\n",
    "| Customer_ID   | String    | Unique customer identifier                 |\n",
    "| Customer_Name | String    | Full name of the customer                  |\n",
    "| Segment       | String    | Market segment (Consumer, Corporate, etc.) |\n",
    "| Country       | String    | Country of the order                       |\n",
    "| City          | String    | City of the order                          |\n",
    "| State         | String    | State or province                          |\n",
    "| Postal_Code   | Integer   | Postal code                                |\n",
    "| Region        | String    | Geographic region                          |\n",
    "| Product_ID    | String    | Product identifier                         |\n",
    "| Category      | String    | Product category                           |\n",
    "| Sub_Category  | String    | Product sub-category                       |\n",
    "| Product_Name  | String    | Product name                               |\n",
    "| Sales         | String    | Sales amount (convert to numeric)          |\n",
    "| Quantity      | String    | Quantity ordered (convert to integer)      |\n",
    "| Discount      | String    | Discount applied                           |\n",
    "| Profit        | Double    | Profit from the sale                       |\n",
    "| User_ID       | Double    | User identifier (if applicable)            |\n",
    "| State_ID      | Double    | State identifier (if applicable)           |\n",
    "| Order_S       | String    | Additional order reference or remarks      |\n",
    "\n",
    "> **Note:** Some numeric fields (e.g., `Sales`, `Quantity`, `Discount`) are currently stored as strings and must be cast to appropriate numeric types during transformation.\n",
    "\n",
    "---\n",
    "\n",
    "###  Data Transformation Steps\n",
    "\n",
    "1. **Data Ingestion**\n",
    "\n",
    "   * Load CSV or Excel data using PySpark (`spark.read.csv`).\n",
    "   * Store the raw data in the **Bronze Layer**.\n",
    "\n",
    "2. **Data Cleaning**\n",
    "\n",
    "   * Handle missing values, duplicates, and invalid records.\n",
    "   * Convert data types (e.g., `Order_Date` ‚Üí `date` using `to_date()`).\n",
    "\n",
    "3. **Data Transformation**\n",
    "\n",
    "   * Compute KPIs using `groupBy()` and aggregation functions.\n",
    "   * Derive new columns such as `year`, `month`, and `quarter` for time-based analysis.\n",
    "\n",
    "4. **Data Enrichment**\n",
    "\n",
    "   * Add regional or categorical hierarchies (Country ‚Üí Region ‚Üí City).\n",
    "   * Aggregate sales and profit by category, customer, and geography.\n",
    "\n",
    "5. **Data Storage**\n",
    "\n",
    "   * Save curated data into the **Silver Layer** (clean data).\n",
    "   * Publish analytical results to the **Gold Layer** for dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "###  Example PySpark KPI Computations\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import sum, countDistinct, col\n",
    "\n",
    "# Total number of customers\n",
    "total_customers = df.select(countDistinct(\"Customer_ID\")).collect()[0][0]\n",
    "\n",
    "# Total orders\n",
    "total_orders = df.select(countDistinct(\"Order_ID\")).collect()[0][0]\n",
    "\n",
    "# Total sales\n",
    "total_sales = df.agg(sum(col(\"Sales\").cast(\"double\"))).collect()[0][0]\n",
    "\n",
    "# Total profit\n",
    "total_profit = df.agg(sum(\"Profit\")).collect()[0][0]\n",
    "\n",
    "# Top sales by country\n",
    "top_country_sales = (\n",
    "    df.groupBy(\"Country\")\n",
    "      .agg(sum(col(\"Sales\").cast(\"double\")).alias(\"Total_Sales\"))\n",
    "      .orderBy(col(\"Total_Sales\").desc())\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Outputs\n",
    "\n",
    "* Databricks SQL or Power BI dashboard with:\n",
    "\n",
    "  * Total Customers, Orders, Sales, and Profit\n",
    "  * Top Performing Countries and Regions\n",
    "  * Top 10 Products by Category and Sub-Category\n",
    "  * Top Customers by Sales per City\n",
    "  * Monthly and Yearly trend visualizations\n",
    "\n",
    "---\n",
    "\n",
    "### üóìÔ∏è Temporal Analysis\n",
    "\n",
    "* **Monthly Analysis:** Track sales, profit, and customer growth per month.\n",
    "* **Quarterly Analysis:** Identify top-performing quarters.\n",
    "* **Yearly Summary:** Compare metrics across years (if available).\n",
    "\n",
    "---\n",
    "\n",
    "### Automation & Refresh\n",
    "\n",
    "* This project uses **historical data**, so no real-time weekly refresh is required.\n",
    "* However, the notebook includes widgets for:\n",
    "\n",
    "  * üìÖ **Date range filtering**\n",
    "  * üóìÔ∏è **Monthly or yearly selection**\n",
    "* The notebook can be easily extended for **scheduled jobs** in case future data ingestion pipelines are added.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32fd020-f15a-4833-aef5-996ebd12e975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SuperstoreAnalysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885e2156-9d41-42da-8947-3d3265be2929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read CSV file\n",
    "df = spark.read.csv(\"/Volumes/workspace/dataset/dataset-store/superstore.csv\", \n",
    "                    header=True,       # use first row as column names\n",
    "                    inferSchema=True)  # automatically infer data types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c31cc3-0801-47e7-b21d-51d79435f9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display all columns \n",
    "# df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9971e3d-d258-4be0-9537-e2128a7247ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepare and Clean  Data\n",
    "- Before answering the questions, make i should have to be sure that date columns and numeric columns are correctly cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f15abdb-a647-4844-ba1d-ee607b20cfb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, expr, col, trim\n",
    "\n",
    "df_clean = (\n",
    "    df.withColumn(\"Order_Date\", \n",
    "        when(col(\"Order_Date\").rlike(r\"^\\d{2}/\\d{2}/\\d{4}$\"), expr(\"try_to_date(Order_Date, 'dd/MM/yyyy')\"))\n",
    "        .when(col(\"Order_Date\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\"), expr(\"try_to_date(Order_Date, 'yyyy-MM-dd')\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    .withColumn(\"Ship _Date\", \n",
    "        when(col(\"Ship _Date\").rlike(r\"^\\d{2}/\\d{2}/\\d{4}$\"), expr(\"try_to_date(`Ship _Date`, 'dd/MM/yyyy')\"))\n",
    "        .when(col(\"Ship _Date\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\"), expr(\"try_to_date(`Ship _Date`, 'yyyy-MM-dd')\"))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    .withColumn(\"Sales\", expr(\"try_cast(trim(Sales) as double)\"))\n",
    "    .withColumn(\"Quantity\", expr(\"try_cast(trim(Quantity) as int)\"))\n",
    "    .withColumn(\"Profit\", expr(\"try_cast(trim(Profit) as double)\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e58c82e-d14f-48d3-9eb8-a6437336a95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Save the cleaned DataFrame as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91eccab0-77d7-4f90-aa4c-a1da2477975d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save cleaned DataFrame as multiple CSV files (folder)\n",
    "df_clean.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .csv(\"/Volumes/workspace/dataset/dataset-store/cleaned_superstore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4990ee21-5c82-4426-8077-caadaeecc7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optional: check schema and preview data\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3eba0a2-e036-409d-9029-d665069d7afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(df_clean.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e8ab78-d5e8-422a-b44d-26b8981f60f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "‚Äî is used in PySpark (and Databricks) to make your DataFrame accessible as a SQL table that can be queried from anywhere within your Spark session (and even across notebooks, if global).\n",
    "\"\"\"\n",
    "# create a global temporary view\n",
    "df.createOrReplaceTempView(\"cleaned_superstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7021f6-cacb-4675-a3d4-0eca075c5ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM cleaned_superstore LIMIT 4;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3a5c22-572b-4a3b-ba16-25f38231e3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT column_name, data_type, is_nullable\n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'cleaned_superstore';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e85838d-3be4-4ec3-b216-5912d4bbd025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE cleaned_superstore;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c18f3cd-d236-4e17-9e22-07564fc2211b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### CLEANING SUPERSTORE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096c2c8d-e438-4e03-a121-cb790f3179a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, avg, when, lit\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Numeric columns stored as strings\n",
    "numeric_columns_str = [\"Sales\", \"Quantity\", \"Discount\"]\n",
    "numeric_columns_double = [\"Profit\", \"user_id\", \"state_id\"]\n",
    "numeric_columns_int = [\"Postal_Code\"]\n",
    "\n",
    "# Text columns to fill nulls with \"Unknown\"\n",
    "text_columns = [\"Order_id\", \"Order_Date\", \"Ship _Date\", \"Ship_Mode\",\n",
    "                \"Customer_id\", \"Customer_Name\", \"Segment\", \"Country\",\n",
    "                \"City\", \"State\", \"Region\", \"Product_ ID\", \"Category\",\n",
    "                \"Sub_Category\", \"Product_Name\", \"order_s\"]\n",
    "\n",
    "# Step 1: Convert numeric string columns to double\n",
    "df_cleaned = df_clean\n",
    "for c in numeric_columns_str:\n",
    "    df_cleaned = df_cleaned.withColumn(\n",
    "        c + \"_num\",\n",
    "        regexp_replace(col(c), \"[^0-9.]\", \"\").cast(DoubleType())\n",
    "    )\n",
    "\n",
    "# Step 2: Fill nulls in numeric columns with their average\n",
    "for c in numeric_columns_str:\n",
    "    avg_value = df_cleaned.select(avg(col(c + \"_num\"))).first()[0]\n",
    "    df_cleaned = df_cleaned.withColumn(\n",
    "        c + \"_num\",\n",
    "        when(col(c + \"_num\").isNull(), lit(avg_value)).otherwise(col(c + \"_num\"))\n",
    "    )\n",
    "\n",
    "for c in numeric_columns_double:\n",
    "    avg_value = df_cleaned.select(avg(col(c))).first()[0]\n",
    "    df_cleaned = df_cleaned.withColumn(\n",
    "        c,\n",
    "        when(col(c).isNull(), lit(avg_value)).otherwise(col(c))\n",
    "    )\n",
    "\n",
    "for c in numeric_columns_int:\n",
    "    avg_value = df_cleaned.select(avg(col(c))).first()[0]\n",
    "    df_cleaned = df_cleaned.withColumn(\n",
    "        c,\n",
    "        when(col(c).isNull(), lit(int(avg_value))).otherwise(col(c))\n",
    "    )\n",
    "\n",
    "# Step 3: Fill nulls in text columns with \"Unknown\"\n",
    "df_cleaned = df_cleaned.fillna(\"Unknown\", subset=text_columns)\n",
    "\n",
    "# Step 4: Optional - drop old numeric string columns if you want\n",
    "# df_cleaned = df_cleaned.drop(*numeric_columns_str)\n",
    "\n",
    "# Step 5: Show top 5 rows\n",
    "df_cleaned.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fb6564-b65c-410e-b7fb-9d0038649de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = df_cleaned.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_cleaned.columns\n",
    "])\n",
    "\n",
    "# null_counts.show(truncate=False)\n",
    "\n",
    "null_counts.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e383df4-c9be-47a1-ad46-b55470f48e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Cleaning Overview: Identifying Missing Values and ‚ÄúUnknown‚Äù Entries in Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d95afe9-9a3f-42b8-8884-cc811577ca95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Only string/text columns\n",
    "string_columns = [c for c, t in df_cleaned.dtypes if t == \"string\"]\n",
    "\n",
    "# Filter rows where any string column has \"Unknown\"\n",
    "df_cleaned.filter(\n",
    "    reduce(lambda a, b: a | b, [(col(c) == \"Unknown\") for c in string_columns])\n",
    ")\n",
    "\n",
    "# Count the number of such rows\n",
    "num_unknown_rows = df_cleaned.count()\n",
    "print(f\"Number of rows with 'Unknown' in any string column: {num_unknown_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91cb7298-9800-46a7-a2f4-2b704b64f744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows where ALL string columns are NOT \"Unknown\"\n",
    "valid_rows = df_cleaned.filter(\n",
    "    ~reduce(lambda a, b: a | b, [(col(c) == \"Unknown\") for c in string_columns])\n",
    ")\n",
    "\n",
    "# Count the number of such rows\n",
    "num_valid_rows = valid_rows.count()\n",
    "print(f\"Number of rows with no 'Unknown' in any string column: {num_valid_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb52233f-fbbd-47e7-8e79-346ca7511fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBmdW5jdG9vbHMgaW1wb3J0IHJlZHVjZQpmcm9tIHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBpbXBvcnQgY29sCgpkZl9jbGVhbmVkLmNyZWF0ZU9yUmVwbGFjZVRlbXBWaWV3KCJjbGVhbmVkX3N1cGVyc3RvcmUiKQoKc3RyaW5nX2NvbHVtbnMgPSBbYyBmb3IgYywgdCBpbiBkZl9jbGVhbmVkLmR0eXBlcyBpZiB0ID09ICJzdHJpbmciXQoKdW5rbm93bl9zcWxfY29uZGl0aW9uID0gIiBPUiAiLmpvaW4oW2YiYHtjfWAgPSAnVW5rbm93biciIGZvciBjIGluIHN0cmluZ19jb2x1bW5zXSkKCnNxbF9xdWVyeSA9IGYiIiIKU0VMRUNUICdSb3dzIFdJVEggVW5rbm93bicgQVMgTWV0cmljLCBDT1VOVCgqKSBBUyBDb3VudApGUk9NIGNsZWFuZWRfc3VwZXJzdG9yZQpXSEVSRSB7dW5rbm93bl9zcWxfY29uZGl0aW9ufQoKVU5JT04gQUxMCgpTRUxFQ1QgJ1Jvd3MgV0lUSE9VVCBVbmtub3duJyBBUyBNZXRyaWMsIENPVU5UKCopIEFTIENvdW50CkZST00gY2xlYW5lZF9zdXBlcnN0b3JlCldIRVJFIE5PVCAoe3Vua25vd25fc3FsX2NvbmRpdGlvbn0pCgpVTklPTiBBTEwKClNFTEVDVCAnVG90YWwgUm93cycgQVMgTWV0cmljLCBDT1VOVCgqKSBBUyBDb3VudApGUk9NIGNsZWFuZWRfc3VwZXJzdG9yZQoiIiIKCmRpc3BsYXkoc3Bhcmsuc3FsKHNxbF9xdWVyeSkpCg==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView25006ab\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView25006ab\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView25006ab\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView25006ab) SELECT `Metric`,SUM(`Count`) `column_df1400b4447` FROM q GROUP BY `Metric`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView25006ab\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Metric",
             "id": "column_df1400b4450"
            },
            "y": [
             {
              "column": "Count",
              "id": "column_df1400b4447",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "clockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_df1400b4447": {
             "name": "Count",
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "af6bbde1-a4f1-4776-bb92-5be9027ad86f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.1950199753046036,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Metric",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Metric",
           "type": "column"
          },
          {
           "alias": "column_df1400b4447",
           "args": [
            {
             "column": "Count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_cleaned.createOrReplaceTempView(\"cleaned_superstore\")\n",
    "\n",
    "string_columns = [c for c, t in df_cleaned.dtypes if t == \"string\"]\n",
    "\n",
    "unknown_sql_condition = \" OR \".join([f\"`{c}` = 'Unknown'\" for c in string_columns])\n",
    "\n",
    "sql_query = f\"\"\"\n",
    "SELECT 'Rows WITH Unknown' AS Metric, COUNT(*) AS Count\n",
    "FROM cleaned_superstore\n",
    "WHERE {unknown_sql_condition}\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'Rows WITHOUT Unknown' AS Metric, COUNT(*) AS Count\n",
    "FROM cleaned_superstore\n",
    "WHERE NOT ({unknown_sql_condition})\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'Total Rows' AS Metric, COUNT(*) AS Count\n",
    "FROM cleaned_superstore\n",
    "\"\"\"\n",
    "\n",
    "display(spark.sql(sql_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3520be-bd02-4249-a54e-e027b457f9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "summary_df = spark.createDataFrame([\n",
    "    (\"Rows WITH 'Unknown'\", count_with_unknown, round(count_with_unknown / total_rows * 100, 2)),\n",
    "    (\"Rows WITHOUT 'Unknown'\", count_without_unknown, round(count_without_unknown / total_rows * 100, 2)),\n",
    "    (\"Total Rows\", total_rows, 100.0)\n",
    "], [\"Metric\", \"Count\", \"Percentage\"])\n",
    "\n",
    "summary_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56bccbf0-2be0-4d97-8265-71f6682d6d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# .drop(*columns_to_drop)\n",
    "df_cleaned.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f4504b-389b-422f-bc82-8965b48dd126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "text_columns = [\"Order_id\", \"Order_Date\", \"Ship _Date\", \"Ship_Mode\",\n",
    "                \"Customer_id\", \"Customer_Name\", \"Segment\", \"Country\",\n",
    "                \"City\", \"State\", \"Region\", \"Product_ ID\", \"Category\",\n",
    "                \"Sub_Category\", \"Product_Name\", \"order_s\"]\n",
    "\n",
    "# Count nulls per text column\n",
    "null_counts_text = df_cleaned.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c + \"_nulls\")\n",
    "    for c in text_columns\n",
    "])\n",
    "\n",
    "null_counts_text.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e514ee1-21c9-41cf-8ec0-b357fc6af2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Show cleaned DataFrame\n",
    "df_cleaned.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0fbf2f-8785-4871-a238-4154cdd57dde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "# Calculate total nulls per column\n",
    "null_counts = df_cleaned.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c + \"_nulls\")\n",
    "    for c in df_cleaned.columns\n",
    "])\n",
    "\n",
    "null_counts.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e764f41d-c863-4fb0-9cf4-f40642ffce2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned.limit(1).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8b3e5d-8c78-4f99-b112-9899d6ae4250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, when, lit\n",
    "\n",
    "# Compute average of Sales, ignoring nulls\n",
    "avg_sales = df_cleaned.select(avg(col(\"Sales\"))).first()[0]\n",
    "\n",
    "# Fill nulls in Sales with the average\n",
    "df_final_cleaned  = df_cleaned.withColumn(\n",
    "    \"Sales\",\n",
    "    when(col(\"Sales\").isNull(), lit(avg_sales)).otherwise(col(\"Sales\"))\n",
    ")\n",
    "\n",
    "# Check that nulls are filled\n",
    "df_final_cleaned.filter(col(\"Sales\").isNull()).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f657f24-b9f0-4d8a-899c-e5ffd217255a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Final Cleaned Superstore Dataset: Nulls Filled & CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab57d4e-4d6a-457d-a1a1-da6a6af5f507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save as multiple CSV files (folder)\n",
    "df_final_cleaned.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .csv(\"/Volumes/workspace/dataset/dataset-store/cleaned_superstore_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15715ea0-8d75-416f-a49a-3624aedeb384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python cell\n",
    "df_final_cleaned.createOrReplaceTempView(\"cleaned_superstore_final_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f031089e-abbf-41b7-a5f3-4b95a615dfde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM cleaned_superstore_final_view\n",
    "LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb88b6e-648c-4f62-8e80-89f6d9916bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1Ô∏è Minimum and Maximum Order_Date\n",
    "SELECT \n",
    "    MIN(Order_Date) AS min_order_date,\n",
    "    MAX(Order_Date) AS max_order_date\n",
    "FROM cleaned_superstore_final_view;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90159397-0f70-4c74-8fe0-90917f489292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Counting the total number of unique customers in the Superstore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d6a40c-8471-467b-aad6-b15f81214f84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 2Ô∏è List all distinct years in the dataset\n",
    "SELECT DISTINCT YEAR(Order_Date) AS order_year\n",
    "FROM cleaned_superstore_final_view\n",
    "ORDER BY order_year;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08e3a26-c931-4e94-b198-bab5fe376ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "\n%sql select count(distinct customer_id)  AS total_customers from cleaned_superstore_final_view",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "counterColName": "total_customers",
           "counterLabel": "Total Customers",
           "rowNumber": 0,
           "stringDecChar": ".",
           "stringDecimal": 0,
           "stringThouSep": ",",
           "targetColName": "",
           "targetRowNumber": 2,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "6a5dba34-0219-4a8a-8a75-f3bc697d484f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.195037841796875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql select count(distinct customer_id)  AS total_customers from cleaned_superstore_final_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e34721-d56d-4b2a-803f-d68340b88c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Counting the number of unique customers who placed orders in the year 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34d658c-bbb0-46a6-b1d4-39b2ab6f30fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "\n%sql\nSELECT COUNT(DISTINCT customer_id) AS total_customers_2017\nFROM cleaned_superstore_final_view\nWHERE order_date BETWEEN '2022-01-01' AND '2022-12-31';",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "countRow": false,
           "counterColName": "total_customers_2017",
           "counterLabel": "Unique Customers who placed order in 2022",
           "formatTargetValue": false,
           "rowNumber": 1,
           "stringDecChar": ".",
           "stringDecimal": 0,
           "stringThouSep": ",",
           "targetRowNumber": 1,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "9093dcd2-85d2-4079-8aa6-769bd7783b73",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.1950531005859375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%sql\n",
    "SELECT COUNT(DISTINCT customer_id) AS total_customers_2017\n",
    "FROM cleaned_superstore_final_view\n",
    "WHERE order_date BETWEEN '2022-01-01' AND '2022-12-31';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b49376-4375-499c-80a6-1c79139f1efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Automate data retrieving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47a7a29-ae3d-431c-8042-b7b9db9308a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Widgets with Date Range and Exception Handling  \n",
    "\n",
    "Databricks does not provide a native ‚Äúdate picker‚Äù widget.  \n",
    "However, we can easily create **text widgets** for selecting **start** and **end** dates while enforcing the `YYYY-MM-DD` format.  \n",
    "\n",
    "In this section, we‚Äôll build a **combined interactive system** that allows you to:  \n",
    "- **Filter data by a specific start and end date**, **or**  \n",
    "- **Select a specific year and month** for monthly reporting.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f87441-ffa7-483b-ada6-ccd85b734feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    # Remove previous widgets safely\n",
    "    dbutils.widgets.removeAll()\n",
    "\n",
    "    # ===============================\n",
    "    # Mode Selector (Choose Filter Type)\n",
    "    # ===============================\n",
    "    dbutils.widgets.dropdown(\n",
    "        name=\"filter_mode\",\n",
    "        defaultValue=\"date_range\",\n",
    "        choices=[\"date_range\", \"monthly\"],\n",
    "        label=\"üîç Select Filter Mode\"\n",
    "    )\n",
    "\n",
    "    filter_mode = dbutils.widgets.get(\"filter_mode\")\n",
    "\n",
    "    # ===============================\n",
    "    #  DATE RANGE FILTER MODE\n",
    "    # ===============================\n",
    "    if filter_mode == \"date_range\":\n",
    "        dbutils.widgets.text(\n",
    "            name=\"start_date\",\n",
    "            defaultValue=\"2023-01-01\",\n",
    "            label=\" Start Date (YYYY-MM-DD)\"\n",
    "        )\n",
    "        dbutils.widgets.text(\n",
    "            name=\"end_date\",\n",
    "            defaultValue=\"2023-12-31\",\n",
    "            label=\" End Date (YYYY-MM-DD)\"\n",
    "        )\n",
    "\n",
    "        start_date = dbutils.widgets.get(\"start_date\")\n",
    "        end_date = dbutils.widgets.get(\"end_date\")\n",
    "\n",
    "        # Validate date inputs\n",
    "        try:\n",
    "            start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "            end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "            if start_date_obj > end_date_obj:\n",
    "                raise ValueError(\"üö´ Start Date cannot be after End Date!\")\n",
    "\n",
    "            print(f\"‚úÖ Date range selected: {start_date} ‚Üí {end_date}\")\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"‚ö†Ô∏è Invalid date input: {ve}\")\n",
    "            raise\n",
    "\n",
    "    # ===============================\n",
    "    #  MONTHLY FILTER MODE\n",
    "    # ===============================\n",
    "    elif filter_mode == \"monthly\":\n",
    "        # Year selector\n",
    "        dbutils.widgets.dropdown(\n",
    "            name=\"year\",\n",
    "            defaultValue=\"2023\",\n",
    "            choices=[\"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"],\n",
    "            label=\" Select Year\"\n",
    "        )\n",
    "\n",
    "        # Month selector\n",
    "        dbutils.widgets.dropdown(\n",
    "            name=\"month\",\n",
    "            defaultValue=\"01\",\n",
    "            choices=[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"],\n",
    "            label=\" Select Month\"\n",
    "        )\n",
    "\n",
    "        year = dbutils.widgets.get(\"year\")\n",
    "        month = dbutils.widgets.get(\"month\")\n",
    "\n",
    "        print(f\"‚úÖ Monthly filter selected: {year}-{month}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating widgets or reading values: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0124f320-634c-4375-8ae1-158ac9252cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Retrieve Widget Values in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbe50ff-4922-4253-89e4-8efd95e7d5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_date = dbutils.widgets.get(\"start_date\")\n",
    "end_date = dbutils.widgets.get(\"end_date\")\n",
    "\n",
    "print(f\"Filtering data between {start_date} and {end_date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9bca836-d208-4a68-9622-4e0c9516a408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Use the Date Range in a SQL Query (Python + Spark SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6e04f4-9448-46c2-90f1-d325e0b2d130",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761934692789}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Metric 1: Total Unique Customers, Sales, and Profit within a date range\n",
    "SELECT  \n",
    "    format_number(COUNT(DISTINCT Customer_ID), 0) AS total_customers,\n",
    "    format_number(SUM(try_cast(Sales AS DOUBLE)), 2) AS total_sales,\n",
    "    format_number(SUM(try_cast(Profit AS DOUBLE)), 2) AS total_profit\n",
    "FROM cleaned_superstore_final_view\n",
    "WHERE Order_Date BETWEEN getArgument('start_date') AND getArgument('end_date');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee107204-0805-42b0-a329-f02e86b838d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Metric 1: Total Unique Customers, Sales, and Profit within a date range\n",
    "SELECT  \n",
    "    format_number(COUNT(DISTINCT Customer_ID), 0) AS total_customers,\n",
    "    format_number(SUM(try_cast(Sales AS DOUBLE)), 2) AS total_sales,\n",
    "    format_number(SUM(try_cast(Profit AS DOUBLE)), 2) AS total_profit\n",
    "FROM cleaned_superstore_final_view\n",
    "WHERE CAST(Order_Date AS DATE) \n",
    "      BETWEEN CAST(getArgument('start_date') AS DATE) \n",
    "      AND CAST(getArgument('end_date') AS DATE);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebe9902-6a50-4fea-af2b-43d055fc0566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"cXVlcnkgPSBmIiIiClNFTEVDVCAKICAgIENPVU5UKERJU1RJTkNUIEN1c3RvbWVyX0lEKSBBUyB0b3RhbF9jdXN0b21lcnMsCiAgICBTVU0oQ09BTEVTQ0UodHJ5X2Nhc3QoU2FsZXMgQVMgRE9VQkxFKSwgMCkpIEFTIHRvdGFsX3NhbGVzLAogICAgU1VNKENPQUxFU0NFKHRyeV9jYXN0KFByb2ZpdCBBUyBET1VCTEUpLCAwKSkgQVMgdG90YWxfcHJvZml0CkZST00gY2xlYW5lZF9zdXBlcnN0b3JlX2ZpbmFsX3ZpZXcKV0hFUkUgT3JkZXJfRGF0ZSBCRVRXRUVOICd7c3RhcnRfZGF0ZX0nIEFORCAne2VuZF9kYXRlfScKIiIiCmRpc3BsYXkoc3Bhcmsuc3FsKHF1ZXJ5KSkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewddee409\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewddee409\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewddee409\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewddee409) SELECT `total_sales`,SUM(`total_customers`) `column_df1400b4477`,SUM(`total_sales`) `column_df1400b4488`,SUM(`total_profit`) `column_df1400b4491`,`total_customers` FROM q GROUP BY `total_customers`,`total_sales`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewddee409\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "total_customers",
             "id": "column_df1400b4493"
            },
            "x": {
             "column": "total_sales",
             "id": "column_df1400b4486"
            },
            "y": [
             {
              "column": "total_customers",
              "id": "column_df1400b4477",
              "transform": "SUM"
             },
             {
              "column": "total_sales",
              "id": "column_df1400b4488",
              "transform": "SUM"
             },
             {
              "column": "total_profit",
              "id": "column_df1400b4491",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "enabled": true,
            "placement": "auto",
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "percentValues": true,
            "stacking": null
           },
           "seriesOptions": {
            "573, SUM(total_customers)": {
             "yAxis": 0
            },
            "573, SUM(total_profit)": {
             "yAxis": 1
            },
            "573, SUM(total_sales)": {
             "yAxis": 1
            },
            "column_df1400b4468": {
             "type": "column",
             "yAxis": 0
            },
            "column_df1400b4471": {
             "type": "column",
             "yAxis": 0
            },
            "column_df1400b4474": {
             "type": "column",
             "yAxis": 0
            },
            "column_df1400b4477": {
             "type": "column",
             "yAxis": 0
            },
            "column_df1400b4488": {
             "type": "column",
             "yAxis": 0
            },
            "column_df1400b4491": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "c3f9df6e-1ef6-4ba5-a7c7-3644aeda819b",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.232177734375,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "total_sales",
           "type": "column"
          },
          {
           "column": "total_customers",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "total_sales",
           "type": "column"
          },
          {
           "alias": "column_df1400b4477",
           "args": [
            {
             "column": "total_customers",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_df1400b4488",
           "args": [
            {
             "column": "total_sales",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_df1400b4491",
           "args": [
            {
             "column": "total_profit",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "total_customers",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT Customer_ID) AS total_customers,\n",
    "    SUM(COALESCE(try_cast(Sales AS DOUBLE), 0)) AS total_sales,\n",
    "    SUM(COALESCE(try_cast(Profit AS DOUBLE), 0)) AS total_profit\n",
    "FROM cleaned_superstore_final_view\n",
    "WHERE Order_Date BETWEEN '{start_date}' AND '{end_date}'\n",
    "\"\"\"\n",
    "display(spark.sql(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c576dca-598c-4690-8833-57f3b9b5bb35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT Customer_ID) AS total_customers,\n",
    "    SUM(try_cast(Sales AS DOUBLE)) AS total_sales,\n",
    "    SUM(try_cast(Profit AS DOUBLE)) AS total_profit\n",
    "FROM cleaned_superstore_final_view\n",
    "WHERE Order_Date BETWEEN '{start_date}' AND '{end_date}'\n",
    "\"\"\"\n",
    "display(spark.sql(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaadc995-eca6-4d66-bc7c-866733d72769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. KPI 1 Customer, Sales, and Profit Overview\n",
    "- Calculates total unique customers, total sales, and total profit, either by date range or by month, with a dynamic, descriptive title.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b957d4f-029e-4db1-8305-8efc6325dfc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Base query\n",
    "base_query = \"\"\"\n",
    "SELECT  \n",
    "    format_number(COUNT(DISTINCT Customer_ID), 0) AS total_customers,\n",
    "    format_number(SUM(try_cast(Sales AS DOUBLE)), 2) AS total_sales,\n",
    "    format_number(SUM(try_cast(Profit AS DOUBLE)), 2) AS total_profit\n",
    "FROM cleaned_superstore_final_view\n",
    "\"\"\"\n",
    "\n",
    "if filter_mode == \"date_range\":\n",
    "    query = f\"\"\"\n",
    "    {base_query}\n",
    "    WHERE Order_Date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    \"\"\"\n",
    "elif filter_mode == \"monthly\":\n",
    "    query = f\"\"\"\n",
    "    {base_query}\n",
    "    WHERE month(Order_Date) = {int(month)} AND year(Order_Date) = {int(year)}\n",
    "    \"\"\"\n",
    "\n",
    "# Execute and display\n",
    "display(spark.sql(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b5739c-e744-4945-8c12-0c84ec93fe3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if filter_mode == \"date_range\":\n",
    "    title = f\" Total Number of Unique Customers, Sales, and Profit from {start_date} to {end_date}\"\n",
    "elif filter_mode == \"monthly\":\n",
    "    title = f\" Total Number of Unique Customers, Sales, and Profit for {year}-{month}\"\n",
    "\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff510c0-6651-416c-a725-89e0292f9889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### KPI Total Unique Customers within a date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "761ab6b9-2c9c-4015-b1c3-7f1c0cc728cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Metric 1: Total Unique Customers within a date range\n",
    "SELECT  \n",
    "    format_number(COUNT(DISTINCT Customer_ID), 0) AS total_customers\n",
    "FROM cleaned_superstore_final_view\n",
    "WHERE Order_Date BETWEEN getArgument('start_date') AND getArgument('end_date');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bcbc85-4c79-4d31-8255-0f204e6fd1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "total_customers = df_final_cleaned.select(countDistinct(\"Customer_id\").alias(\"unique_customers\")).first()[\"unique_customers\"]\n",
    "\n",
    "print(\"\\n Metric 1: Total Unique Customers\")\n",
    "print(f\"   {total_customers:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b705acee-d67e-429b-885e-d602b25d2885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load your data (adjust table name)\n",
    "df = spark.table(\"cleaned_superstore_final_view\")  # \n",
    "\n",
    "# Apply dynamic date filtering\n",
    "df_filtered = df.filter(\n",
    "    (col(\"order_date\") >= start_date) & \n",
    "    (col(\"order_date\") <= end_date)\n",
    ")\n",
    "\n",
    "# Show record count after filtering\n",
    "filtered_count = df_filtered.count()\n",
    "print(f\"\\n Total records in selected period: {filtered_count:,}\\n\")\n",
    "\n",
    "if filtered_count == 0:\n",
    "    print(\" No data found for the selected period!\")\n",
    "    dbutils.notebook.exit(\"No data available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9c20f1-2eaa-4b7f-8aac-f6c88fe7ee6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### METRIC 2: Total Number of Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bae08cd-bc54-47ff-8d5e-b3af81668d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# METRIC 2: Total Number of Orders\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 2: Total Number of Orders\")\n",
    "print(\"=\"*70)\n",
    "total_orders = df_final_cleaned.select(countDistinct(\"Order_id\")).collect()[0][0]\n",
    "print(f\"Total Orders: {total_orders:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c391b0e8-dc41-4dbd-ae11-3869cad3101a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql\nSELECT \n    'Total Orders' AS Metric,\n    format_number(COUNT(DISTINCT Order_ID), 0) AS total_orders\nFROM cleaned_superstore_final_view;\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "counterColName": "total_orders",
           "counterLabel": "Total Orders",
           "rowNumber": 1,
           "stringDecChar": ".",
           "stringDecimal": 0,
           "stringThouSep": ",",
           "targetRowNumber": 1,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "9ca68b29-e87f-4af5-a66d-ed42d120fd9f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.763671875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    'Total Orders' AS Metric,\n",
    "    format_number(COUNT(DISTINCT Order_ID), 0) AS total_orders\n",
    "FROM cleaned_superstore_final_view;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9553b51-6497-4c0a-acea-e6b7f616d3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# METRIC 2: Total Number of Orders using SQL\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 2: Total Number of Orders\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use Spark SQL to get total orders\n",
    "total_orders_sql = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT Order_id) AS total_orders\n",
    "    FROM cleaned_superstore_final_view\n",
    "\"\"\").collect()[0][\"total_orders\"]\n",
    "\n",
    "print(f\"Total Orders: {total_orders_sql:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b2e120-dac1-4515-8138-d1f4bd353c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from superstore limit 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5607aba9-1174-419e-bef5-f31276dee844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Repair the Sales column by trying to extract numeric parts (if any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73722508-9f39-4d64-a2f4-686c59db2585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get first 100 Sales values\n",
    "SELECT Sales\n",
    "FROM cleaned_superstore_final_view\n",
    "LIMIT 100;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059835f0-fda2-410d-bfeb-eaf31ffdbe2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get first 100 sales values\n",
    "sales_list = [row.Sales for row in df_final_cleaned.select(\"Sales\").limit(100).collect()]\n",
    "print(sales_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73cce5c-5752-45c7-b8c0-1f4c80248eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# ===============================\n",
    "# METRIC 3: Total Sales Amount\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 3: Total Sales Amount\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_sales = df_final_cleaned.select(_sum(\"Sales\")).collect()[0][0]\n",
    "\n",
    "if total_sales is None:\n",
    "    total_sales = 0\n",
    "print(f\"Total Sales: ${total_sales:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad79b405-25ec-445d-8bd0-559229cadadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql\nSELECT \n    'Total Sales' AS Metric,\n    FORMAT_NUMBER(SUM(TRY_CAST(Sales AS DOUBLE)), 2) AS total_sales\nFROM cleaned_superstore_final_view;\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "COUNTER"
         },
         {
          "key": "options",
          "value": {
           "counterColName": "total_sales",
           "counterLabel": "Total Sales",
           "rowNumber": 1,
           "stringDecChar": ".",
           "stringDecimal": 0,
           "stringThouSep": ",",
           "targetColName": "",
           "targetRowNumber": 1,
           "tooltipFormat": "0,0.000"
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "implicitDf": true,
        "rowLimit": 10000
       },
       "nuid": "1dccf1cc-08dc-4fcc-a98b-f289adfbfb82",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.998046875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    'Total Sales' AS Metric,\n",
    "    FORMAT_NUMBER(SUM(TRY_CAST(Sales AS DOUBLE)), 2) AS total_sales\n",
    "FROM cleaned_superstore_final_view;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91f6638-3d05-41a9-8946-d8e542676f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### To avoid collect()[0][0] by using .first() and a column alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543cc894-c8da-4d72-9328-826f69cfbfac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure the DataFrame is accessible as a SQL view\n",
    "df_final_cleaned.createOrReplaceTempView(\"cleaned_superstore_final_view\")\n",
    "\n",
    "# SQL query to sum Sales\n",
    "query = \"\"\"\n",
    "SELECT SUM(Sales) AS total_sales\n",
    "FROM cleaned_superstore_final_view\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "total_sales = spark.sql(query).collect()[0][\"total_sales\"]\n",
    "\n",
    "# Display with formatting\n",
    "print(f\"Total Sales: ${total_sales:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e64f2934-ee39-4659-86b0-4fe8aff3e37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# METRIC 4: Total Profit\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 4: Total Profit\")\n",
    "print(\"=\"*70)\n",
    "total_profit = df_final_cleaned.select(_sum(\"profit\")).collect()[0][0] # \n",
    "if total_profit is None:\n",
    "    total_profit = 0\n",
    "profit_margin = (total_profit / total_sales * 100) if total_sales > 0 else 0\n",
    "print(f\"Total Profit: ${total_profit:,.2f}\")\n",
    "print(f\"Profit Margin: {profit_margin:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba3743c-5b80-4cba-a62d-b36136795315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    FORMAT_NUMBER(SUM(TRY_CAST(Profit AS DOUBLE)), 2) AS total_profit,\n",
    "    FORMAT_NUMBER(SUM(TRY_CAST(Sales AS DOUBLE)), 2) AS total_sales,\n",
    "    ROUND(\n",
    "        CASE WHEN SUM(TRY_CAST(Sales AS DOUBLE)) > 0 \n",
    "             THEN SUM(TRY_CAST(Profit AS DOUBLE)) / SUM(TRY_CAST(Sales AS DOUBLE)) * 100\n",
    "             ELSE 0\n",
    "        END,\n",
    "        2\n",
    "    ) AS profit_margin_percentage\n",
    "FROM cleaned_superstore_final_view;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3215ca5d-0005-47dc-b8d2-ec97500bb55f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum, col\n",
    "\n",
    "# Use the existing filter_mode and date/month widgets\n",
    "filter_mode = dbutils.widgets.get(\"filter_mode\")\n",
    "\n",
    "if filter_mode == \"date_range\":\n",
    "    start_date = dbutils.widgets.get(\"start_date\")\n",
    "    end_date = dbutils.widgets.get(\"end_date\")\n",
    "    \n",
    "    df_filtered_dynamic = df_final_cleaned.filter(\n",
    "        (col(\"Order_Date\") >= start_date) & (col(\"Order_Date\") <= end_date)\n",
    "    )\n",
    "\n",
    "elif filter_mode == \"monthly\":\n",
    "    year = dbutils.widgets.get(\"year\")\n",
    "    month = dbutils.widgets.get(\"month\")\n",
    "    \n",
    "    # Filter by month and year\n",
    "    df_filtered_dynamic = df_final_cleaned.filter(\n",
    "        (col(\"Order_Date\").substr(1, 4) == year) &\n",
    "        (col(\"Order_Date\").substr(6, 2) == month)\n",
    "    )\n",
    "\n",
    "# ===============================\n",
    "# Metric 4: Total Profit (dynamic)\n",
    "# ===============================\n",
    "total_profit = df_filtered_dynamic.select(_sum(\"Profit\")).collect()[0][0] or 0\n",
    "total_sales = df_filtered_dynamic.select(_sum(\"Sales\")).collect()[0][0] or 0\n",
    "\n",
    "profit_margin = (total_profit / total_sales * 100) if total_sales > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 4: Total Profit (Dynamic)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total Profit: ${total_profit:,.2f}\")\n",
    "print(f\"Profit Margin: {profit_margin:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3441bc8e-6446-4a71-9abc-44d91ed1db15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### METRIC 5: Top Sales by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197e89a2-2234-4f48-b916-520ee76f0270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, count\n",
    "\n",
    "# ===============================\n",
    "# METRIC 5: Top Sales by Country\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 5: Top Sales by Country\")\n",
    "print(\"=\"*70)\n",
    "sales_by_country = df_filtered.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"sales\").alias(\"total_sales\"),\n",
    "        count(\"order_id\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "display(sales_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b21ced-4c05-44e4-9fec-484b373721d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_mode = dbutils.widgets.get(\"filter_mode\")\n",
    "\n",
    "if filter_mode == \"date_range\":\n",
    "    start_date = dbutils.widgets.get(\"start_date\")\n",
    "    end_date = dbutils.widgets.get(\"end_date\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        Country,\n",
    "        FORMAT_NUMBER(SUM(TRY_CAST(Sales AS DOUBLE)), 2) AS total_sales,\n",
    "        COUNT(Order_ID) AS order_count\n",
    "    FROM cleaned_superstore_final_view\n",
    "    WHERE Order_Date BETWEEN '{start_date}' AND '{end_date}'\n",
    "    GROUP BY Country\n",
    "    ORDER BY total_sales DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "elif filter_mode == \"monthly\":\n",
    "    year = dbutils.widgets.get(\"year\")\n",
    "    month = dbutils.widgets.get(\"month\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        Country,\n",
    "        FORMAT_NUMBER(SUM(TRY_CAST(Sales AS DOUBLE)), 2) AS total_sales,\n",
    "        COUNT(Order_ID) AS order_count\n",
    "    FROM cleaned_superstore_final_view\n",
    "    WHERE YEAR(Order_Date) = {year} AND MONTH(Order_Date) = {month}\n",
    "    GROUP BY Country\n",
    "    ORDER BY total_sales DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "display(spark.sql(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08919740-76c0-4714-8065-e9814fd45eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### METRIC 6: Most Profitable Region and Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d876dd11-393f-496e-bbb4-2e970319d95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# METRIC 6: Most Profitable Region and Country\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 6: Most Profitable Region and Country\")\n",
    "print(\"=\"*70)\n",
    "most_profitable = df_final_cleaned.groupBy(\"region\", \"country\") \\\n",
    "    .agg(\n",
    "        _sum(\"profit\").alias(\"total_profit\"),\n",
    "        _sum(\"sales\").alias(\"total_sales\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_profit\")) \\\n",
    "    .limit(5)\n",
    "\n",
    "display(most_profitable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af52ec4-a351-4609-bc04-06e8e03511b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    region,\n",
    "    country,\n",
    "    SUM(profit) AS total_profit,\n",
    "    SUM(sales) AS total_sales\n",
    "FROM cleaned_superstore_final_view\n",
    "GROUP BY region, country\n",
    "ORDER BY total_profit DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "display(spark.sql(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0840df-3745-4db4-ae08-d981059331d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### METRIC 7: Top Sales by Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa02827-cf88-45bd-8d7b-d6ac4274562d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# METRIC 7: Top Sales by Product Category\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 7: Top Sales by Product Category\")\n",
    "print(\"=\"*70)\n",
    "sales_by_category = df_final_cleaned.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        _sum(\"sales\").alias(\"total_sales\"),\n",
    "        _sum(\"profit\").alias(\"total_profit\"),\n",
    "        count(\"order_id\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "display(sales_by_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d499b0a-c9fe-4675-88c6-b24cd6e0458d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### METRIC 8: Top 10 Products by Sub-Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a8b72a4-ddf2-448b-99a9-df2371f6c0cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, sum as _sum\n",
    "\n",
    "# ===============================\n",
    "# METRIC 8: Top 10 Products by Sub-Category\n",
    "# ===============================\n",
    "\n",
    "# Remove non-numeric characters (keep digits and decimal point)\n",
    "df_cleaned = df_final_cleaned.withColumn(\n",
    "    \"Sales_clean\",\n",
    "    regexp_replace(col(\"Sales\"), \"[^0-9.]\", \"\")\n",
    ")\n",
    "\n",
    "# Convert to float/double\n",
    "df_cleaned = df_cleaned.withColumn(\"Sales_double\", col(\"Sales_clean\").cast(\"double\"))\n",
    "\n",
    "# Aggregate top 10 sub-categories\n",
    "top10 = df_cleaned.groupBy(\"Sub_Category\").agg(_sum(\"Sales_double\").alias(\"total_sales\"))\n",
    "top10.orderBy(col(\"total_sales\").desc()).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0118770f-9c44-492b-8898-a2195ff8602a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### METRIC 9: Most Frequently Ordered Product (by Quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6782d7b4-4408-44bb-9de5-aa3cd4098ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get first 100 Sales values\n",
    "SELECT *\n",
    "FROM cleaned_superstore_final_view\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88404a20-9c36-4818-a9db-aa2757e09e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================== \n",
    "# METRIC 9: Most Frequently Ordered Product (by Quantity) \n",
    "# ===============================\n",
    "from pyspark.sql.functions import col, regexp_replace, sum as _sum, count, desc, coalesce, lit, when\n",
    "\n",
    "# Aggregate top products safely\n",
    "most_ordered_product = df_final_cleaned.groupBy(\"Product_Name\", \"Category\") \\\n",
    "    .agg(\n",
    "        _sum(\"Quantity\").alias(\"total_quantity\"),\n",
    "        count(\"Order_id\").alias(\"order_count\"),\n",
    "        _sum(\"Sales\").alias(\"total_sales\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_quantity\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "display(most_ordered_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d31c8d-ea62-4fac-9ea8-c61acdde8ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### METRIC 10: Top Customer by Total Sales per City\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7ff787-b74d-46a9-b8d9-85f950fb74a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, sum as _sum, count, desc, col\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# METRIC 10: Top Customer by Total Sales per City\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" METRIC 10: Top Customer by Total Sales per City\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate total sales per customer per city\n",
    "customer_sales_by_city = df_final_cleaned.groupBy(\"city\", \"customer_id\", \"customer_name\") \\\n",
    "    .agg(\n",
    "        _sum(\"sales\").alias(\"total_sales\"),\n",
    "        count(\"order_id\").alias(\"order_count\")\n",
    "    )\n",
    "\n",
    "# Rank customers within each city\n",
    "window_spec = Window.partitionBy(\"city\").orderBy(desc(\"total_sales\"))\n",
    "top_customer_per_city = customer_sales_by_city.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") == 1) \\\n",
    " .drop(\"rank\") \\\n",
    " .orderBy(desc(\"total_sales\")) \\\n",
    " .limit(20)\n",
    "\n",
    "display(top_customer_per_city)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ea29ed-4fff-43d4-9c7a-4ecad13b5130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### FINAL SUMMARY DASHBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae4318c-411f-47a5-9a98-96f479427d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "#  FINAL SUMMARY DASHBOARD\n",
    "# ===============================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìÖ Period: {start_date} to {end_date}\")\n",
    "print(f\"üîç Filter Mode: {filter_mode.upper()}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"üë• Unique Customers:     {total_customers:,}\")\n",
    "print(f\"üì¶ Total Orders:         {total_orders:,}\")\n",
    "print(f\"üí∞ Total Sales:          ${total_sales:,.2f}\")\n",
    "print(f\"üíµ Total Profit:         ${total_profit:,.2f}\")\n",
    "print(f\"üìä Profit Margin:        {profit_margin:.2f}%\")\n",
    "if total_orders > 0:\n",
    "    print(f\"üõí Avg Order Value:      ${(total_sales/total_orders):,.2f}\")\n",
    "    print(f\"üë§ Avg Sales/Customer:   ${(total_sales/total_customers):,.2f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "dd2b7727-a3f6-4954-8a0c-a97f84543e3e",
     "origId": 8494747508877756,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7662191833438652,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks-pyspark-project",
   "widgets": {
    "end_date": {
     "currentValue": "2022-12-31",
     "nuid": "cb08ebda-94fc-40be-97cb-ef9d02f46d5c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2023-12-31",
      "label": " End Date (YYYY-MM-DD)",
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2023-12-31",
      "label": " End Date (YYYY-MM-DD)",
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "filter_mode": {
     "currentValue": "date_range",
     "nuid": "c10af0f3-a442-4b29-9b11-25b20f2585bc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "date_range",
      "label": "üîç Select Filter Mode",
      "name": "filter_mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "date_range",
        "monthly"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "date_range",
      "label": "üîç Select Filter Mode",
      "name": "filter_mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "date_range",
        "monthly"
       ]
      }
     }
    },
    "start_date": {
     "currentValue": "2022-01-01",
     "nuid": "5b337218-34d1-4f80-83fa-47c613e4bfe3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2023-01-01",
      "label": " Start Date (YYYY-MM-DD)",
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2023-01-01",
      "label": " Start Date (YYYY-MM-DD)",
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
